{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f814da20910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 준다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[3], [5], [7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "# 가중치 b를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# 가중치 W를 출력\n",
    "print(W) \n",
    "# 가중치 b를 출력\n",
    "print(b) \n",
    "\n",
    "\n",
    "## 현재 가중치 W와 b 둘다 0이므로 현 직선의 방정식은 y = 0 * x + 0d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 가설 세우기\n",
    "hypothesis = W * x_train + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 비용함수 선언 / 평균제곱오차 MSE\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법의 일종인 SGD 구현 lr == learning rate\n",
    "optimizer = optim.SGD([W, b], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화. \n",
    "# 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있다.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산된다.\n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward()\n",
    "\n",
    "#  그 다음 경사 하강법 최적화 함수 opimizer의 .step() 함수를 호출하여 \n",
    "#  인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) \n",
    "#  0.001을 곱하여 빼줌으로서 업데이트한다. \n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20000 W: 0.660, b: 0.099 Cost: 2854.500000\n",
      "Epoch  100/20000 W: 8.346, b: 1.614 Cost: 18.894985\n",
      "Epoch  200/20000 W: 8.293, b: 2.000 Cost: 17.372295\n",
      "Epoch  300/20000 W: 8.240, b: 2.371 Cost: 15.972483\n",
      "Epoch  400/20000 W: 8.189, b: 2.726 Cost: 14.685478\n",
      "Epoch  500/20000 W: 8.140, b: 3.066 Cost: 13.502174\n",
      "Epoch  600/20000 W: 8.093, b: 3.392 Cost: 12.414219\n",
      "Epoch  700/20000 W: 8.048, b: 3.705 Cost: 11.413931\n",
      "Epoch  800/20000 W: 8.005, b: 4.005 Cost: 10.494231\n",
      "Epoch  900/20000 W: 7.963, b: 4.293 Cost: 9.648649\n",
      "Epoch 1000/20000 W: 7.924, b: 4.569 Cost: 8.871189\n",
      "Epoch 1100/20000 W: 7.886, b: 4.833 Cost: 8.156383\n",
      "Epoch 1200/20000 W: 7.849, b: 5.087 Cost: 7.499172\n",
      "Epoch 1300/20000 W: 7.814, b: 5.330 Cost: 6.894916\n",
      "Epoch 1400/20000 W: 7.781, b: 5.564 Cost: 6.339346\n",
      "Epoch 1500/20000 W: 7.749, b: 5.787 Cost: 5.828540\n",
      "Epoch 1600/20000 W: 7.718, b: 6.002 Cost: 5.358901\n",
      "Epoch 1700/20000 W: 7.688, b: 6.207 Cost: 4.927096\n",
      "Epoch 1800/20000 W: 7.660, b: 6.404 Cost: 4.530089\n",
      "Epoch 1900/20000 W: 7.633, b: 6.593 Cost: 4.165076\n",
      "Epoch 2000/20000 W: 7.607, b: 6.775 Cost: 3.829466\n",
      "Epoch 2100/20000 W: 7.582, b: 6.948 Cost: 3.520905\n",
      "Epoch 2200/20000 W: 7.558, b: 7.115 Cost: 3.237201\n",
      "Epoch 2300/20000 W: 7.535, b: 7.275 Cost: 2.976362\n",
      "Epoch 2400/20000 W: 7.513, b: 7.428 Cost: 2.736537\n",
      "Epoch 2500/20000 W: 7.492, b: 7.575 Cost: 2.516036\n",
      "Epoch 2600/20000 W: 7.472, b: 7.716 Cost: 2.313304\n",
      "Epoch 2700/20000 W: 7.452, b: 7.851 Cost: 2.126903\n",
      "Epoch 2800/20000 W: 7.434, b: 7.981 Cost: 1.955528\n",
      "Epoch 2900/20000 W: 7.416, b: 8.105 Cost: 1.797957\n",
      "Epoch 3000/20000 W: 7.399, b: 8.224 Cost: 1.653083\n",
      "Epoch 3100/20000 W: 7.382, b: 8.338 Cost: 1.519875\n",
      "Epoch 3200/20000 W: 7.367, b: 8.448 Cost: 1.397410\n",
      "Epoch 3300/20000 W: 7.352, b: 8.553 Cost: 1.284809\n",
      "Epoch 3400/20000 W: 7.337, b: 8.653 Cost: 1.181286\n",
      "Epoch 3500/20000 W: 7.323, b: 8.750 Cost: 1.086105\n",
      "Epoch 3600/20000 W: 7.310, b: 8.842 Cost: 0.998589\n",
      "Epoch 3700/20000 W: 7.297, b: 8.931 Cost: 0.918127\n",
      "Epoch 3800/20000 W: 7.285, b: 9.016 Cost: 0.844146\n",
      "Epoch 3900/20000 W: 7.273, b: 9.098 Cost: 0.776129\n",
      "Epoch 4000/20000 W: 7.262, b: 9.176 Cost: 0.713592\n",
      "Epoch 4100/20000 W: 7.251, b: 9.251 Cost: 0.656093\n",
      "Epoch 4200/20000 W: 7.241, b: 9.323 Cost: 0.603227\n",
      "Epoch 4300/20000 W: 7.231, b: 9.392 Cost: 0.554623\n",
      "Epoch 4400/20000 W: 7.221, b: 9.458 Cost: 0.509933\n",
      "Epoch 4500/20000 W: 7.212, b: 9.522 Cost: 0.468844\n",
      "Epoch 4600/20000 W: 7.204, b: 9.582 Cost: 0.431069\n",
      "Epoch 4700/20000 W: 7.195, b: 9.641 Cost: 0.396334\n",
      "Epoch 4800/20000 W: 7.187, b: 9.697 Cost: 0.364398\n",
      "Epoch 4900/20000 W: 7.180, b: 9.750 Cost: 0.335035\n",
      "Epoch 5000/20000 W: 7.172, b: 9.802 Cost: 0.308041\n",
      "Epoch 5100/20000 W: 7.165, b: 9.851 Cost: 0.283220\n",
      "Epoch 5200/20000 W: 7.158, b: 9.898 Cost: 0.260399\n",
      "Epoch 5300/20000 W: 7.152, b: 9.943 Cost: 0.239416\n",
      "Epoch 5400/20000 W: 7.146, b: 9.987 Cost: 0.220124\n",
      "Epoch 5500/20000 W: 7.140, b: 10.029 Cost: 0.202389\n",
      "Epoch 5600/20000 W: 7.134, b: 10.069 Cost: 0.186081\n",
      "Epoch 5700/20000 W: 7.128, b: 10.107 Cost: 0.171088\n",
      "Epoch 5800/20000 W: 7.123, b: 10.144 Cost: 0.157304\n",
      "Epoch 5900/20000 W: 7.118, b: 10.179 Cost: 0.144629\n",
      "Epoch 6000/20000 W: 7.113, b: 10.213 Cost: 0.132976\n",
      "Epoch 6100/20000 W: 7.108, b: 10.245 Cost: 0.122260\n",
      "Epoch 6200/20000 W: 7.104, b: 10.276 Cost: 0.112409\n",
      "Epoch 6300/20000 W: 7.100, b: 10.306 Cost: 0.103352\n",
      "Epoch 6400/20000 W: 7.096, b: 10.334 Cost: 0.095024\n",
      "Epoch 6500/20000 W: 7.092, b: 10.362 Cost: 0.087368\n",
      "Epoch 6600/20000 W: 7.088, b: 10.388 Cost: 0.080328\n",
      "Epoch 6700/20000 W: 7.084, b: 10.413 Cost: 0.073856\n",
      "Epoch 6800/20000 W: 7.081, b: 10.437 Cost: 0.067906\n",
      "Epoch 6900/20000 W: 7.077, b: 10.460 Cost: 0.062434\n",
      "Epoch 7000/20000 W: 7.074, b: 10.483 Cost: 0.057404\n",
      "Epoch 7100/20000 W: 7.071, b: 10.504 Cost: 0.052779\n",
      "Epoch 7200/20000 W: 7.068, b: 10.524 Cost: 0.048526\n",
      "Epoch 7300/20000 W: 7.066, b: 10.544 Cost: 0.044616\n",
      "Epoch 7400/20000 W: 7.063, b: 10.563 Cost: 0.041022\n",
      "Epoch 7500/20000 W: 7.060, b: 10.581 Cost: 0.037717\n",
      "Epoch 7600/20000 W: 7.058, b: 10.598 Cost: 0.034678\n",
      "Epoch 7700/20000 W: 7.055, b: 10.614 Cost: 0.031884\n",
      "Epoch 7800/20000 W: 7.053, b: 10.630 Cost: 0.029315\n",
      "Epoch 7900/20000 W: 7.051, b: 10.646 Cost: 0.026953\n",
      "Epoch 8000/20000 W: 7.049, b: 10.660 Cost: 0.024782\n",
      "Epoch 8100/20000 W: 7.047, b: 10.674 Cost: 0.022785\n",
      "Epoch 8200/20000 W: 7.045, b: 10.687 Cost: 0.020949\n",
      "Epoch 8300/20000 W: 7.043, b: 10.700 Cost: 0.019261\n",
      "Epoch 8400/20000 W: 7.041, b: 10.713 Cost: 0.017710\n",
      "Epoch 8500/20000 W: 7.040, b: 10.724 Cost: 0.016283\n",
      "Epoch 8600/20000 W: 7.038, b: 10.736 Cost: 0.014971\n",
      "Epoch 8700/20000 W: 7.036, b: 10.747 Cost: 0.013765\n",
      "Epoch 8800/20000 W: 7.035, b: 10.757 Cost: 0.012656\n",
      "Epoch 8900/20000 W: 7.033, b: 10.767 Cost: 0.011636\n",
      "Epoch 9000/20000 W: 7.032, b: 10.777 Cost: 0.010699\n",
      "Epoch 9100/20000 W: 7.031, b: 10.786 Cost: 0.009837\n",
      "Epoch 9200/20000 W: 7.029, b: 10.795 Cost: 0.009044\n",
      "Epoch 9300/20000 W: 7.028, b: 10.803 Cost: 0.008316\n",
      "Epoch 9400/20000 W: 7.027, b: 10.811 Cost: 0.007646\n",
      "Epoch 9500/20000 W: 7.026, b: 10.819 Cost: 0.007030\n",
      "Epoch 9600/20000 W: 7.025, b: 10.826 Cost: 0.006463\n",
      "Epoch 9700/20000 W: 7.024, b: 10.834 Cost: 0.005943\n",
      "Epoch 9800/20000 W: 7.023, b: 10.840 Cost: 0.005464\n",
      "Epoch 9900/20000 W: 7.022, b: 10.847 Cost: 0.005023\n",
      "Epoch 10000/20000 W: 7.021, b: 10.853 Cost: 0.004619\n",
      "Epoch 10100/20000 W: 7.020, b: 10.859 Cost: 0.004247\n",
      "Epoch 10200/20000 W: 7.019, b: 10.865 Cost: 0.003905\n",
      "Epoch 10300/20000 W: 7.019, b: 10.871 Cost: 0.003590\n",
      "Epoch 10400/20000 W: 7.018, b: 10.876 Cost: 0.003301\n",
      "Epoch 10500/20000 W: 7.017, b: 10.881 Cost: 0.003035\n",
      "Epoch 10600/20000 W: 7.016, b: 10.886 Cost: 0.002791\n",
      "Epoch 10700/20000 W: 7.016, b: 10.891 Cost: 0.002566\n",
      "Epoch 10800/20000 W: 7.015, b: 10.895 Cost: 0.002359\n",
      "Epoch 10900/20000 W: 7.014, b: 10.899 Cost: 0.002169\n",
      "Epoch 11000/20000 W: 7.014, b: 10.904 Cost: 0.001994\n",
      "Epoch 11100/20000 W: 7.013, b: 10.908 Cost: 0.001834\n",
      "Epoch 11200/20000 W: 7.013, b: 10.911 Cost: 0.001686\n",
      "Epoch 11300/20000 W: 7.012, b: 10.915 Cost: 0.001550\n",
      "Epoch 11400/20000 W: 7.012, b: 10.918 Cost: 0.001425\n",
      "Epoch 11500/20000 W: 7.011, b: 10.922 Cost: 0.001310\n",
      "Epoch 11600/20000 W: 7.011, b: 10.925 Cost: 0.001205\n",
      "Epoch 11700/20000 W: 7.010, b: 10.928 Cost: 0.001108\n",
      "Epoch 11800/20000 W: 7.010, b: 10.931 Cost: 0.001019\n",
      "Epoch 11900/20000 W: 7.009, b: 10.934 Cost: 0.000937\n",
      "Epoch 12000/20000 W: 7.009, b: 10.937 Cost: 0.000861\n",
      "Epoch 12100/20000 W: 7.009, b: 10.939 Cost: 0.000792\n",
      "Epoch 12200/20000 W: 7.008, b: 10.942 Cost: 0.000728\n",
      "Epoch 12300/20000 W: 7.008, b: 10.944 Cost: 0.000669\n",
      "Epoch 12400/20000 W: 7.008, b: 10.946 Cost: 0.000615\n",
      "Epoch 12500/20000 W: 7.007, b: 10.949 Cost: 0.000566\n",
      "Epoch 12600/20000 W: 7.007, b: 10.951 Cost: 0.000520\n",
      "Epoch 12700/20000 W: 7.007, b: 10.953 Cost: 0.000478\n",
      "Epoch 12800/20000 W: 7.007, b: 10.955 Cost: 0.000440\n",
      "Epoch 12900/20000 W: 7.006, b: 10.957 Cost: 0.000405\n",
      "Epoch 13000/20000 W: 7.006, b: 10.958 Cost: 0.000372\n",
      "Epoch 13100/20000 W: 7.006, b: 10.960 Cost: 0.000342\n",
      "Epoch 13200/20000 W: 7.005, b: 10.962 Cost: 0.000315\n",
      "Epoch 13300/20000 W: 7.005, b: 10.963 Cost: 0.000289\n",
      "Epoch 13400/20000 W: 7.005, b: 10.965 Cost: 0.000266\n",
      "Epoch 13500/20000 W: 7.005, b: 10.966 Cost: 0.000245\n",
      "Epoch 13600/20000 W: 7.005, b: 10.968 Cost: 0.000225\n",
      "Epoch 13700/20000 W: 7.004, b: 10.969 Cost: 0.000207\n",
      "Epoch 13800/20000 W: 7.004, b: 10.970 Cost: 0.000190\n",
      "Epoch 13900/20000 W: 7.004, b: 10.971 Cost: 0.000175\n",
      "Epoch 14000/20000 W: 7.004, b: 10.973 Cost: 0.000161\n",
      "Epoch 14100/20000 W: 7.004, b: 10.974 Cost: 0.000148\n",
      "Epoch 14200/20000 W: 7.004, b: 10.975 Cost: 0.000136\n",
      "Epoch 14300/20000 W: 7.003, b: 10.976 Cost: 0.000125\n",
      "Epoch 14400/20000 W: 7.003, b: 10.977 Cost: 0.000115\n",
      "Epoch 14500/20000 W: 7.003, b: 10.978 Cost: 0.000106\n",
      "Epoch 14600/20000 W: 7.003, b: 10.979 Cost: 0.000097\n",
      "Epoch 14700/20000 W: 7.003, b: 10.980 Cost: 0.000089\n",
      "Epoch 14800/20000 W: 7.003, b: 10.980 Cost: 0.000082\n",
      "Epoch 14900/20000 W: 7.003, b: 10.981 Cost: 0.000076\n",
      "Epoch 15000/20000 W: 7.003, b: 10.982 Cost: 0.000070\n",
      "Epoch 15100/20000 W: 7.002, b: 10.983 Cost: 0.000064\n",
      "Epoch 15200/20000 W: 7.002, b: 10.983 Cost: 0.000059\n",
      "Epoch 15300/20000 W: 7.002, b: 10.984 Cost: 0.000054\n",
      "Epoch 15400/20000 W: 7.002, b: 10.985 Cost: 0.000050\n",
      "Epoch 15500/20000 W: 7.002, b: 10.985 Cost: 0.000046\n",
      "Epoch 15600/20000 W: 7.002, b: 10.986 Cost: 0.000042\n",
      "Epoch 15700/20000 W: 7.002, b: 10.987 Cost: 0.000039\n",
      "Epoch 15800/20000 W: 7.002, b: 10.987 Cost: 0.000036\n",
      "Epoch 15900/20000 W: 7.002, b: 10.988 Cost: 0.000033\n",
      "Epoch 16000/20000 W: 7.002, b: 10.988 Cost: 0.000030\n",
      "Epoch 16100/20000 W: 7.002, b: 10.989 Cost: 0.000028\n",
      "Epoch 16200/20000 W: 7.002, b: 10.989 Cost: 0.000026\n",
      "Epoch 16300/20000 W: 7.001, b: 10.990 Cost: 0.000023\n",
      "Epoch 16400/20000 W: 7.001, b: 10.990 Cost: 0.000021\n",
      "Epoch 16500/20000 W: 7.001, b: 10.990 Cost: 0.000020\n",
      "Epoch 16600/20000 W: 7.001, b: 10.991 Cost: 0.000018\n",
      "Epoch 16700/20000 W: 7.001, b: 10.991 Cost: 0.000017\n",
      "Epoch 16800/20000 W: 7.001, b: 10.992 Cost: 0.000015\n",
      "Epoch 16900/20000 W: 7.001, b: 10.992 Cost: 0.000014\n",
      "Epoch 17000/20000 W: 7.001, b: 10.992 Cost: 0.000013\n",
      "Epoch 17100/20000 W: 7.001, b: 10.992 Cost: 0.000012\n",
      "Epoch 17200/20000 W: 7.001, b: 10.993 Cost: 0.000011\n",
      "Epoch 17300/20000 W: 7.001, b: 10.993 Cost: 0.000010\n",
      "Epoch 17400/20000 W: 7.001, b: 10.993 Cost: 0.000009\n",
      "Epoch 17500/20000 W: 7.001, b: 10.994 Cost: 0.000009\n",
      "Epoch 17600/20000 W: 7.001, b: 10.994 Cost: 0.000008\n",
      "Epoch 17700/20000 W: 7.001, b: 10.994 Cost: 0.000007\n",
      "Epoch 17800/20000 W: 7.001, b: 10.994 Cost: 0.000007\n",
      "Epoch 17900/20000 W: 7.001, b: 10.995 Cost: 0.000006\n",
      "Epoch 18000/20000 W: 7.001, b: 10.995 Cost: 0.000006\n",
      "Epoch 18100/20000 W: 7.001, b: 10.995 Cost: 0.000005\n",
      "Epoch 18200/20000 W: 7.001, b: 10.995 Cost: 0.000005\n",
      "Epoch 18300/20000 W: 7.001, b: 10.995 Cost: 0.000005\n",
      "Epoch 18400/20000 W: 7.001, b: 10.996 Cost: 0.000004\n",
      "Epoch 18500/20000 W: 7.001, b: 10.996 Cost: 0.000004\n",
      "Epoch 18600/20000 W: 7.001, b: 10.996 Cost: 0.000003\n",
      "Epoch 18700/20000 W: 7.001, b: 10.996 Cost: 0.000003\n",
      "Epoch 18800/20000 W: 7.001, b: 10.996 Cost: 0.000003\n",
      "Epoch 18900/20000 W: 7.000, b: 10.997 Cost: 0.000003\n",
      "Epoch 19000/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19100/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19200/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19300/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19400/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19500/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19600/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19700/20000 W: 7.000, b: 10.997 Cost: 0.000002\n",
      "Epoch 19800/20000 W: 7.000, b: 10.997 Cost: 0.000001\n",
      "Epoch 19900/20000 W: 7.000, b: 10.998 Cost: 0.000001\n",
      "Epoch 20000/20000 W: 7.000, b: 10.998 Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "x_trainSet = torch.FloatTensor([1,2,3,4,5,6,7,8,9,10])\n",
    "y_trainSet = torch.FloatTensor([18,25,32,39,46,53,60,67,74,81])\n",
    "\n",
    "W_2 = torch.zeros(1,requires_grad=True)\n",
    "b_2 = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "hypothesis_2 = W_2 * x_trainSet + b_2\n",
    "\n",
    "cost_2 = torch.mean((hypothesis_2 - y_trainSet) ** 2)\n",
    "\n",
    "optimizer_2 = optim.SGD([W_2, b_2], lr=0.001)\n",
    "\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    # 가설 계산\n",
    "    hypothesis_2 = W_2 * x_trainSet + b_2\n",
    "    \n",
    "    # Cost 계산\n",
    "    cost_2 = torch.mean((hypothesis_2 - y_trainSet) **2)\n",
    "    \n",
    "    # cost 로 Hx 개선\n",
    "    optimizer_2.zero_grad()\n",
    "    cost_2.backward()\n",
    "    optimizer_2.step()\n",
    "    \n",
    "    # 100번마다 로그출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, W_2.item(), b_2.item(), cost_2.item()\n",
    "        ))\n",
    "        \n",
    " # 위에 정답은 7x + 11 이었는데,  w= 7, b = 10.998 로 cost 도 0.000001임을 알 수 있다.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbfa867c80980d085025f7ec622c1a9d9521b336ebdba89779c207447b7ffca6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
